# -*- coding: utf-8 -*-
"""DecisionTreefromscratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wWhUfO3DLzq3FDDtFhlWrWoNRWxcQcWd
"""

import numpy as np
import pandas as pd

# Define the Decision Tree class
class DecisionTree:
    # Initialize the Decision Tree with minimum samples to split and maximum depth
    def __init__(self, min_samples_split=2, max_depth=5):
        self.min_samples_split = min_samples_split
        self.max_depth = max_depth
        self.tree = None

    # Fit the model on the given dataset
    def fit(self, X, y):
        dataset = np.c_[X, y]
        self.tree = self._build_tree(dataset)

    # Recursively build the decision tree
    def _build_tree(self, dataset, depth=0):
        X, y = dataset[:, :-1], dataset[:, -1]
        n_samples, n_features = X.shape
        if n_samples >= self.min_samples_split and depth < self.max_depth:
            best_split = self._get_best_split(dataset, n_features)
            if best_split:
                left_subtree = self._build_tree(best_split['left'], depth + 1)
                right_subtree = self._build_tree(best_split['right'], depth + 1)
                return Node(best_split['feature_index'], best_split['threshold'], left_subtree, right_subtree)
        leaf_value = self._calculate_leaf_value(y)
        return Node(value=leaf_value)

    # Get the best split for the dataset based on information gain
    def _get_best_split(self, dataset, n_features):
        best_split = {}
        max_info_gain = -float("inf")
        for feature_index in range(n_features):
            feature_values = dataset[:, feature_index]
            possible_thresholds = np.unique(feature_values)
            for threshold in possible_thresholds:
                left, right = self._split(dataset, feature_index, threshold)
                if len(left) > 0 and len(right) > 0:
                    y, left_y, right_y = dataset[:, -1], left[:, -1], right[:, -1]
                    curr_info_gain = self._information_gain(y, left_y, right_y)
                    if curr_info_gain > max_info_gain:
                        max_info_gain = curr_info_gain
                        best_split = {
                            "feature_index": feature_index,
                            "threshold": threshold,
                            "left": left,
                            "right": right
                        }
        return best_split

    # Split the dataset based on a feature and threshold
    def _split(self, dataset, feature_index, threshold):
        left = np.array([row for row in dataset if row[feature_index] <= threshold])
        right = np.array([row for row in dataset if row[feature_index] > threshold])
        return left, right

    # Calculate the information gain
    def _information_gain(self, parent, l_child, r_child):
        weight_l = len(l_child) / len(parent)
        weight_r = len(r_child) / len(parent)
        gain = self._entropy(parent) - (weight_l * self._entropy(l_child) + weight_r * self._entropy(r_child))
        return gain

    # Calculate the entropy of a set of labels
    def _entropy(self, y):
        hist = np.bincount(y.astype(int))
        ps = hist / len(y)
        return -np.sum([p * np.log2(p) for p in ps if p > 0])

    # Calculate the value for a leaf node
    def _calculate_leaf_value(self, y):
        return np.argmax(np.bincount(y.astype(int)))

    # Predict the class for a given set of samples
    def predict(self, X):
        return [self._predict(inputs) for inputs in X]

    # Predict the class for a single sample
    def _predict(self, inputs):
        node = self.tree
        while node.value is None:
            if inputs[node.feature_index] <= node.threshold:
                node = node.left
            else:
                node = node.right
        return node.value

# Define the Node class used in the Decision Tree
class Node:
    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):
        self.feature_index = feature_index
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value

# Example usage
if __name__ == "__main__":
    # Sample dataset
    data = {
        'FeatureA': [2, 1, 3, 2],
        'FeatureB': [3, 4, 2, 1],
        'Label': [0, 0, 1, 1]
    }
    df = pd.DataFrame(data)
    X = df[['FeatureA', 'FeatureB']].values
    y = df['Label'].values

    # Train the decision tree
    clf = DecisionTree(min_samples_split=2, max_depth=3)
    clf.fit(X, y)

    # Predict
    predictions = clf.predict(X)
    print("Predictions:", predictions)